### redo log/bin log

1. redo log的概念是什么? 为什么会存在.
2. 什么是WAL(write-ahead log)机制, 好处是什么.
3. redo log 为什么可以保证crash safe机制.
4. binlog的概念是什么, 起到什么作用, 可以做crash safe吗? 
5. binlog和redolog的不同点有哪些? 
6. 物理一致性和逻辑一致性各应该怎么理解? 
7. 执行器和innoDB在执行update语句时候的流程是什么样的?
8. 如果数据库误操作, 如何执行数据恢复?
9. 什么是两阶段提交, 为什么需要两阶段提交, 两阶段提交怎么保证数据库中两份日志间的逻辑一致性(什么叫逻辑一致性)?
10. 如果不是两阶段提交, 先写redo log和先写bin log两种情况各会遇到什么问题?

### 事务

1. 事务的特性：原子性、一致性、隔离性、持久性
2. 多事务同时执行的时候，可能会出现的问题：脏读、不可重复读、幻读
3. 事务隔离级别：读未提交、读提交、可重复读、串行化
4. 不同事务隔离级别的区别：
   读未提交：一个事务还未提交，它所做的变更就可以被别的事务看到
   读提交：一个事务提交之后，它所做的变更才可以被别的事务看到
   可重复读：一个事务执行过程中看到的数据是一致的。未提交的更改对其他事务是不可见的
   串行化：对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行
5. 配置方法：启动参数transaction-isolation
6. 事务隔离的实现：每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。
7. 回滚日志什么时候删除？系统会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。
8. 什么时候不需要了？当系统里么有比这个回滚日志更早的read-view的时候。
9. 为什么尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图，在这个事务提交之前，回滚记录都要保留，这会导致大量占用存储空间。除此之外，长事务还占用锁资源，可能会拖垮库。
10. 事务启动方式：一、显式启动事务语句，begin或者start transaction,提交commit，回滚rollback；二、set autocommit=0，该命令会把这个线程的自动提交关掉。这样只要执行一个select语句，事务就启动，并不会自动提交，直到主动执行commit或rollback或断开连接。
11. 建议使用方法一，如果考虑多一次交互问题，可以使用commit work and chain语法。在autocommit=1的情况下用begin显式启动事务，如果执行commit则提交事务。如果执行commit work and chain则提交事务并自动启动下一个事务。
12. 在开发过程中，尽可能的减小事务范围，少用长事务，如果无法避免，保证逻辑日志空间足够用，并且支持动态日志空间增长。监控Innodb_trx表，发现长事务报警。
13. 如何避免长事务对业务的影响？
    1. **从应用开发端来看：**
       1. 确认是否使用了set autocommit=0。这个确认工作可以在测试环境中开展，把MySQL的general_log开起来，然后随便跑一个业务逻辑，通过general_log的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成1。
       2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用begin/commit框起来。我见过有些是业务并没有这个需要，但是也把好几个select语句放到了事务中。这种只读事务可以去掉。
       3. 业务连接数据库的时候，根据业务本身的预估，通过SET MAX_EXECUTION_TIME命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）
    2.  **从数据库端来看：**
       1. 监控 information_schema.Innodb_trx表，设置长事务阈值，超过就报警/或者kill；
       2. Percona的pt-kill这个工具不错，推荐使用；
       3. 在业务功能测试阶段要求输出所有的general_log，分析日志行为提前发现问题；
       4. 如果使用的是MySQL 5.6或者更新版本，把innodb_undo_tablespaces设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

### 索引

1. 索引为什么不使用二叉树

   1. 索引不止存在内存中，还要写到磁盘上。

2. 使用N叉树

   1. 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。
   2. N是可以再MySQL中是可以被人工调整的。
   3. 5.6以后可以通过page大小来间接控制

3. 根据叶子节点的内容，索引类型分为主键索引和非主键索引。

   1. 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。
   2. 非主键索引的叶子节点内容是<code>主键的值</code>。在InnoDB里，非主键索引也被称为二级索引（secondary index）。
   3. 基于非主键索引的查询需要多扫描一棵索引树

4. 主键设计

   1. 自增主键：

      1. 每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。
      2. 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。
   
2. 采用业务字段直接作为主键的场景：
  
   1. 有一个索引；
      2. 该索引必须是唯一索引。
   3. 尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。
   
4. 没有主键的表，innodb会给默认创建一个Rowid做主键
  
5. 索引的作用：提高数据查询效率

6. 常见索引模型：哈希表、有序数组、搜索树

7. 哈希表：键 - 值(key - value)。

8. 哈希思路：把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置

9. 哈希冲突的处理办法：链表

10. 哈希表适用场景：只有等值查询的场景

11. 有序数组：按顺序存储。查询用二分法就可以快速查询，时间复杂度是：O(log(N))

12. 有序数组查询效率高，更新效率低

13. 有序数组的适用场景：静态存储引擎。

14. 二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子;查询时间复杂度O(log(N))，更新时间复杂度O(log(N));数据库存储大多不适用二叉树，因为树高过高，会适用N叉树

15. InnoDB中的索引模型：B+Tree

16. 索引类型：主键索引、非主键索引
    主键索引的叶子节点存的是整行的数据(聚簇索引)，非主键索引的叶子节点内容是主键的值(二级索引)

17. 主键索引和普通索引的区别：主键索引只要搜索ID这个B+Tree即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表)

18. 一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。

19. 从性能和存储空间方面考量，<code>自增主键</code>往往是更合理的选择。

20. 回到主键索引树搜索的过程，我们称为回表

23. 覆盖索引

    1. 某索引已经覆盖了查询需求，称为覆盖索引，例如：select ID from T where k between 3 and 5
    2. 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

24. B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。

25. 在建立联合索引的时候，如何安排索引内的字段顺序？

    1. 

26. 索引下推

    <blockquote>select * from tuser where name like '张%' and age=10 and ismale=1;<br>
      根据前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录ID3。当然，这还不错，总比全表扫描要好。然后再判断其他条件是否满足。
    在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。
    而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

27.  InnoDB 这种引擎导致的,虽然删除了表的部分记录,但是它的索引还在, 并未释放。只能是重新建表才能重建索引。

28. 有时候如果在where条件建立索引的效率差的情况下,在order by limit这一列建索引也是很好的方案,排好序,在回表,只要过滤出满足条件的limit行,就能及时停止扫描

29. 业内流传的有一些mysql 军规

    1. 待查 ...

### 锁

1. 根据加锁的范围，MySQL里面的锁大致可以分成<strong>全局锁</strong>、<strong>表级锁</strong>和<strong>行锁</strong>三类。

2. 全局锁

   1. Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候。
   2. **典型使用场景是，做全库逻辑备份。**也就是把整库每个表都select出来存成文本。

3. 表锁

   1. 有两种：

      - 表锁;

        1-表锁的语法是 lock tables … read/write。**与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了<code>本线程</code>接下来的操作对象。

        2-举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。表级别write锁，对于本线程是可读可写的，

      - 元数据锁（meta data lock，MDL)。

        MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。

        当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

        **如何安全地给小表加字段？**

        ​    比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。MariaDB已经合并了AliSQL的这个功能，所以这两个开源分支目前都支持DDL NOWAIT/WAIT n这个语法。

4. 行锁

   1. 在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是<code>两阶段锁协议</code>。
      1. 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量<code>往后放</code>。减少锁的持有时间。
   2. 死锁与死锁检测
      1. 出现死锁以后，有两种策略：
         - 一种策略: 直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。
         - 另一种策略:  启动死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。
         - 正常情况下我们还是要采用<code>第二种策略</code>，即：主动死锁检测。
         - 主动死锁检测：
           - 弊端：每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到CPU利用率很高，但是每秒却执行不了几个事务。
           - 解决方案：
             - **思路-1  ::: 如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉**
             - **思路-2  ::: 另一个思路是控制并发度**;  实现基本思路:对于相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。
   3. innodb行级锁是通过<code>锁索引记录</code>实现的。如果update的列没建索引，即使只update一条记录也会锁定整张表吗？

#### Summary

根据加锁范围：MySQL里面的锁可以分为：全局锁、表级锁、行级锁

一、全局锁：
对整个数据库实例加锁。
MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL)
这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。
使用场景：全库逻辑备份。
风险：
1.如果在主库备份，在备份期间不能更新，业务停摆
2.如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟
官方自带的逻辑备份工具mysqldump，当mysqldump使用参数--single-transaction的时候，会启动一个事务，确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。

一致性读是好，但是前提是引擎要支持这个隔离级别。
如果要全库只读，为什么不使用set global readonly=true的方式？
1.在有些系统中，readonly的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大。
2.在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。
二、表级锁
MySQL里面表级锁有两种，一种是表锁，一种是元数据所(meta data lock,MDL)
表锁的语法是:lock tables ... read/write
可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。表级别write锁，对于本线程是可读可写的。
对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。
MDL：不需要显式使用，在访问一个表的时候会被自动加上。
MDL的作用：保证读写的正确性。
在对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。
读锁之间不互斥。读写锁之间，写锁之间是互斥的，用来保证变更表结构操作的安全性。
MDL 会直到事务提交才会释放，在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。

三、行锁

两阶段锁：在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放， 而是要等到事务结束时才释放。
建议：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。
死锁：当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态。
解决方案：
1、通过参数 innodb_lock_wait_timeout 根据实际业务场景来设置超时时间，InnoDB引擎默认值是50s。
2、发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑（默认是开启状态）。
如何解决热点行更新导致的性能问题？
1、如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关闭掉。一般不建议采用
2、控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。
3、将热更新的行数据拆分成逻辑上的多行来减少锁冲突，但是业务复杂度可能会大大提高。

innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。

### 隔离等级

在MySQL里，有两个“视图”的概念：

- 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view ... ，而它的查询方法与表一样。

- 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

-  在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于<code>整库</code>的。

- InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id。

- 具体实现：

  - InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，<code>启动了但还没提交</code>。

    数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

    这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

    而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。

    这个视图数组把所有的row trx_id 分成了几种不同的情况。    ![img](./images/trx-high-water.png)

​    对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况
   a. 若 row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见。
   b. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。    

  基于上述: InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。

​    一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建<strong>前</strong>提交的，可见。

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**当前读。其实，除了update语句外，select语句如果加锁，也是当前读。

#### Summary
1.innodb支持RC和RR隔离级别实现是用的一致性视图(consistent read view)
2.事务在启动时会拍一个快照,这个快照是基于整个库的.
基于整个库的意思就是说一个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)
如果在事务内select t表,另外的事务执行了DDL t表,根据发生时间,要嘛锁住要嘛报错(参考第六章)
3.事务是如何实现的MVCC呢?
(1)每个事务都有一个事务ID,叫做transaction id(严格递增)
(2)事务在启动时,找到已提交的最大事务ID记为up_limit_id。
(3)事务在更新一条语句时,比如id=1改为了id=2.会把id=1和该行之前的row trx_id写到undo log里,并且在数据页上把id的值改为2,并且把修改这条语句的transaction id记在该行行头
(4)再定一个规矩,一个事务要查看一条数据时,必须先用该事务的up_limit_id与该行的transaction id做比对,
如果up_limit_id>=transaction id,那么可以看.如果up_limit_id<transaction id,则只能去<code>undo log</code>里去取。去undo log查找数据的时候,也需要做比对,必须up_limit_id>transaction id,才返回数据
4.什么是当前读,由于当前读都是先读后写,只能读当前的值,所以为当前读.会更新事务内的up_limit_id为该事务的transaction id
5.为什么rr能实现可重复读而rc不能,分两种情况
(1)快照读的情况下,rr不能更新事务内的up_limit_id,
而rc每次会把up_limit_id更新为快照读之前最新已提交事务的transaction id,则rc不能可重复读
(2)当前读的情况下,rr是利用<code>record lock+gap lock</code>来实现的,而rc没有gap,所以rc不能可重复读    

### 普通索引/唯一索引

- <strong>普通索引</strong>: 查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。

- <strong>唯一索引</strong>: 由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

- <strong>查询场景</strong>: 两者性能上相差微乎其微。 理由：InnoDB的数据是按数据页为单位来读写。对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。

- <strong>更新场景</strong>

  - 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在<strong>change buffer</strong>中，这样就不需要从磁盘中读入这个数据页了。它是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。

    将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。

    显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。

  - change buffer只限于用在普通索引的场景下，而不适用于唯一索引。

  - change bugger机制适用于写多读少的业务场景，不适合读多写少的业务场景(一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。);

- <strong>索引选择实践</strong>

  - 这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。建议尽量选择普通索引。

    如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能。

    在实际使用中，你会发现，普通索引和change buffer的配合使用，对于数据量大的表的更新优化还是很明显的。

    特别地，在使用机械硬盘时，change buffer这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。

​      **redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。**

#### Summary
选择普通索引还是唯一索引？
对于查询过程来说：
a、普通索引，查到满足条件的第一个记录后，继续查找下一个记录，知道第一个不满足条件的记录
b、唯一索引，由于索引唯一性，查到第一个满足条件的记录后，停止检索
但是，两者的性能差距微乎其微。因为InnoDB根据数据页来读写的。
对于更新过程来说：
概念：change buffer
当需要更新一个数据页，如果数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中。下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中的与这个页有关的操作。

change buffer是可以持久化的数据。在内存中有拷贝，也会被写入到磁盘上

purge:将change buffer中的操作应用到原数据页上，得到<strong>最新</strong>结果的过程，成为purge
访问这个数据页会触发purge，系统有后台线程定期purge，在数据库正常关闭的过程中，也会执行purge

唯一索引的更新不能使用change buffer

change buffer用的是buffer pool里的内存，change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。

将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。
change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显。

change buffer使用场景
在一个数据页做purge之前，change buffer记录的变更越多，收益就越大。
对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。

反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer,但之后由于马上要访问这个数据页，会立即触发purge过程。
这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。

索引的选择和实践：
	尽可能使用普通索引。
	redo log主要节省的是随机写磁盘的IO消耗(转成顺序写)，而change buffer主要节省的则是随机读磁盘的IO消耗。

#### MySQL是怎样得到索引的基数的呢

1. Mysql通过<code>采样统计</code>方式，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。
2. 在MySQL中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent的值来选择：

   - 设置为on的时候，表示统计信息会<code>持久化存储</code>。这时，默认的N是20，M是10。
   - 设置为off的时候，表示统计信息只存储在<code>内存</code>中。这时，默认的N是8，M是16。
3. 统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。在实践中，如果你发现explain的结果预估的rows值跟实际情况差距比较大，可以采用这个方法来处理。

#### 索引选择异常和处理

1. 方案一：采用force index强行选择一个索引。
2. 方案二：考虑修改语句，引导MySQL使用我们期望的索引。
3. 方案三：在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。

#### 字符串字段创建索引方式：

1. 直接创建完整索引，这样可能比较占用空间；
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。

###  InnoDB刷脏页

#### 以下两种情况刷脏页都是会明显影响性能

- 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
- 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。

####  刷盘速度要参考以下两个因素

1. 一个是脏页比例。
2. 一个是redo log写盘速度。

合理地设置innodb_io_capacity的值，并且**平时要多关注脏页比例，不要让它经常接近75%**。

###  数据库表的空间回收

1. 一个InnoDB表包含两部分，即：表结构定义和数据

2. <strong>表数据</strong>既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数innodb_file_per_table控制的。

3. InnoDB的数据是按<strong>页</strong>存储的，

   1. 如果删掉了一个数据页上的所有记录，整个数据页就可以被复用了（标记为可复用）。如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。

   2. 如果用delete命令把整个表的数据删除: 所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。delete命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过delete命令是不能回收表空间的。
   3. 如果数据是按照<strong>索引递增顺序插入</strong>的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。

   经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。`重建表`，就可以达到这样的目的。

4. 在重建表的时候，InnoDB不会把整张表占满，每个页留了1/16给后续的更新用。

### count(*)

- MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高；
- InnoDB引擎执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。
- show table status命令虽然返回很快，但是不准确(采样统计得来)；
- 按照效率排序的话，count(字段)<count(主键id)<count(1)≈ count(\*)，所以我建议你，尽量使用count(*)。
  - 统计累加的动作是在server层计算。
  - count(\*)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。
- count(*)性能优化方案
  - 用另一张表记录。读取的时候采用事务。

### 排序  order by

1. MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。

   1. 把所有符合条件的记录，复制到sort_buffer，根据字段排序，返回limit条数的记录。 这种算法为`全字段排序`

   - 若记录字段太大，只将参与排序的字段以及主键放到sort_buffer中进行排序。这个算法成为`rowid排序`

2. 排序可能在`内存`中完成，也可能需要使用`外部`排序，这取决于排序所需的内存和参数sort_buffer_size。

   sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

3.  全字段排序 VS rowid排序
   如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。

   如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。

   这也就体现了MySQL的一个设计思想：**如果内存够，就要多利用内存，尽量减少磁盘访问。**

   对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

4. 如果参与排序的字段，(name,age)，本身就可以从索引中获取。则无需临时表，也不需要排序，直接返回。

**对于InnoDB表来说，执行全字段排序会减少磁盘访问，因此会被优先选择。**

<strong>数据类型转换，就需要走全索引扫描？</strong>例子：select * from tradelog where tradeid=110717;，隐式转化为：select * from tradelog where  CAST(tradid AS signed int) = 110717;

**对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**
rr模式与rc模式的区别：
​	唯一的区别是insert也等待了,是因为rr模式下对没有索引的更新,<strong>聚簇索引上的所有记录，都被加上了X锁</strong>。其次，聚簇索引每条记录间的间隙(GAP)，也同时被加上了GAP锁。由于gap锁阻塞了insert要加的插入意向锁,导致insert也处于等待状态。只有当session 1 commit完成以后。

### 幻读
 	1.  幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。
 	2.  产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。
 	3.  解决幻读问题，InnoDB采用<strong>间隙锁(Gap Lock)</strong>。间隙锁，锁的就是两个值之间的空隙。在一行行扫描的过程中，不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁。
 	4.  **跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。**间隙锁之间都不存在冲突关系(两个相互独立，潜在死锁)。
 	5.  间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。
 	6.  间隙锁在可重复读隔离级别下才有效

<strong>间隙锁和行锁合称next-key lock</strong>

### 加锁规则
#### 加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。
1. 原则1：加锁的<strong>基本单位是next-key lock</strong>。希望你还记得，next-key lock是前开后闭区间。
   1. 由于表t中没有id=7的记录。加锁单位是next-key lock，session A加锁范围就是(5,10]；
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
   1. 等值查询(id=7)，而id=10不满足查询条件，next-key lock退化成间隙锁，因此最终加锁的范围是(5,10)。
5. 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。

分析加锁规则的时候可以用next-key lock来分析。具体执行的时候，是要分成间隙锁和行锁两段来执行的。锁就是加在索引上的

### MySQL写入binlog和redo log的流程。

#### binlog的写入机制

1. binlog的写入逻辑：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。

   1. 每个线程有自己binlog cache，但是共用同一份binlog文件。写binlog共有以下两个步骤

      - write，指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。
      - fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为fsync才占磁盘的IOPS。

      write 和fsync的时机，是由参数sync_binlog控制的：

      1. sync_binlog=0的时候，表示每次提交事务都只write，不fsync；
      2. sync_binlog=1的时候，表示每次提交事务都会执行fsync；
      3. sync_binlog=N(N>1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。
      4. 经验值 sync_binlog设置为100~1000中的某个数值。将sync_binlog设置为N，对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog日志。

#### redo log的写入机制

1. 事务在执行过程中，生成的redo log是要先写到redo log buffer的。

   1. 如果事务执行期间MySQL发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。

2. redo log可能存在的三种状态

   1. 在redo log buffer中，物理上是在MySQL进程内存中；
   2. 写到磁盘(write)，但是没有持久化（fsync)，物理上是在文件系统的page cache里面；
   3. 持久化到磁盘，对应的是hard disk；

3. 控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数，它有三种可能取值：

   1. 设置为0的时候，表示每次事务提交时都只是把redo log留在redo log buffer中;
   2. 设置为1的时候，表示每次事务提交时都将redo log直接持久化到磁盘；
   3. 设置为2的时候，表示每次事务提交时都只是把redo log写到page cache。

   InnoDB有一个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cache，然后调用fsync持久化到磁盘。

   注意，事务执行中间过程的redo log也是直接写在redo log buffer中的，这些redo log也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的redo log，也是可能已经持久化到磁盘的。

### 主从同步

备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的：

1. 在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。
2. 在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。
3. 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。
4. 备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。
5. sql_thread读取中转日志，解析出日志里的命令，并执行。